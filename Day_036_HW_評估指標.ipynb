{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回歸問題\n",
    "    * 常見的評估指標有\n",
    "        觀察「預測值」 (Prediction) 與「實際值」 (Ground truth) 的差距\n",
    "        -MAE: 範圍 [-∞,∞]\n",
    "        -MSE: 範圍 [-∞,∞]\n",
    "        -R-square: 範圍 [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料\n",
    "X, y = datasets.make_regression(n_features=1, random_state=42, noise=4) # 生成資料\n",
    "model = LinearRegression() # 建立回歸模型\n",
    "model.fit(X, y) # 將資料放進模型訓練\n",
    "prediction = model.predict(X) # 進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.8417972525655673\n",
      "MSE:  12.488680067398239\n",
      "R-square:  0.9916581036260311\n"
     ]
    }
   ],
   "source": [
    "# 評估\n",
    "mae = metrics.mean_absolute_error(prediction, y) # 使用 MAE 評估\n",
    "mse = metrics.mean_squared_error(prediction, y) # 使用 MSE 評估\n",
    "r2 = metrics.r2_score(prediction, y) # 使用 r-square 評估\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"R-square: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類問題\n",
    "    * 常見的評估指標有\n",
    "        觀察「預測值」 (prediction) 與「實際值」 (Ground truth) 的正確程度\n",
    "        -AUC: 範圍 [0, 1]\n",
    "        -F1-Score (Precision, Recall): 範圍 [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.評估指標 - 分類 - AUC, Area Under Curve\n",
    "\n",
    "    - AUC 指摽是分類問題常⽤的指標，通常分類問題都需要定⼀個閾值(threshold) \n",
    "        來決定分類的類別 (通常為機率 > 0.5 判定為 1, 機率 < 0.5 判定為 0)\n",
    "    - AUC 是衡量曲線下的⾯積，因此可考量所有閾值下的準確性，因此 AUC也廣泛地在分類問題的比賽中使⽤\n",
    "    \n",
    "    \n",
    "### 2.評估指標 - 分類 - F1-Score\n",
    "    - 分類問題中，我們有時會對某⼀類別的準確率特別有興趣\n",
    "    - Precision，Recall 則是針對某類別進⾏評估\n",
    "        • Precision: 模型判定True，樣本確實為True的比例\n",
    "        • Recall: 模型判定的False，佔樣本所有False的比例\n",
    "    - F1-Score 則是 Precision, Recall 的調和平均數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料\n",
    "cancer = datasets.load_breast_cancer() # 我們使用 sklearn 內含的乳癌資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=50, random_state=0)\n",
    "y_pred = np.random.random((50,)) # 我們先隨機生成 50 筆預測值，範圍都在 0~1 之間，代表機率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.5466893039049237\n"
     ]
    }
   ],
   "source": [
    "# AUC評估\n",
    "auc = metrics.roc_auc_score(y_test, y_pred) # 使用 roc_auc_score 來評估。 **這邊特別注意 y_pred 必須要放機率值進去!**\n",
    "print(\"AUC: \", auc) # 得到結果約 0.5，與亂猜的結果相近，因為我們的預測值是用隨機生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.5964912280701755\n",
      "Precision:  0.6538461538461539\n",
      "Recall:  0.5483870967741935\n"
     ]
    }
   ],
   "source": [
    "# F1評估\n",
    "threshold = 0.5 \n",
    "y_pred_binarized = np.where(y_pred>threshold, 1, 0) # 使用 np.where 函數, 將 y_pred > 0.5 的值變為 1，小於 0.5 的為 0\n",
    "f1 = metrics.f1_score(y_test, y_pred_binarized) # 使用 F1-Score 評估\n",
    "precision = metrics.precision_score(y_test, y_pred_binarized) # 使用 Precision 評估\n",
    "recall  = metrics.recall_score(y_test, y_pred_binarized) # 使用 recall 評估\n",
    "print(\"F1-Score: \", f1) \n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2-Score:  0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "# F2評估\n",
    "beta = 2\n",
    "scoreF2 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "print(\"F2-Score: \", scoreF2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
